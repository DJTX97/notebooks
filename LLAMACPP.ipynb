{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
    "%%html\n",
    "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
    "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Backend & Deps\n",
    "!apt update\n",
    "!apt install aria2 -y\n",
    "!apt install -y build-essential cmake\n",
    "!pip install py-localtunnel\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "\n",
    "# Optional: set environment variable to enable CUDA support (for GPU)\n",
    "import os\n",
    "os.environ[\"LLAMA_CUBLAS\"] = \"1\"  # Optional: remove if using CPU\n",
    "\n",
    "# Build the project (detects LLAMA_CUBLAS if set)\n",
    "!make server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download Model\n",
    "OPTIONS = {\n",
    "    1: {\n",
    "        \"id\": \"mradermacher/Fimbulvetr-11B-v2-GGUF\",\n",
    "        \"file\": \"https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-GGUF/blob/main/Fimbulvetr-11B-v2.Q5_K_S.gguf\"\n",
    "    },\n",
    "    2: {\n",
    "        \"id\": \"mradermacher/MN-LooseCannon-12B-v2-GGUF\",\n",
    "        \"file\": \"https://huggingface.co/mradermacher/MN-LooseCannon-12B-v2-GGUF/blob/main/MN-LooseCannon-12B-v2.Q6_K.gguf\"\n",
    "    },\n",
    "    3: {\n",
    "        \"id\": \"bartowski/Dans-PersonalityEngine-V1.1.0-12b-GGUF\",\n",
    "        \"file\": \"https://huggingface.co/bartowski/Dans-PersonalityEngine-V1.1.0-12b-GGUF/blob/main/Dans-PersonalityEngine-V1.1.0-12b-Q6_K.gguf\"\n",
    "    },\n",
    "    4: {\n",
    "        \"id\":\"mradermacher/Magnum-Picaro-0.7-v2-12b-i1-GGUF\",\n",
    "        \"file\":\"https://huggingface.co/mradermacher/Magnum-Picaro-0.7-v2-12b-i1-GGUF/blob/main/Magnum-Picaro-0.7-v2-12b.i1-Q6_K.gguf\"\n",
    "    },\n",
    "    5: {\n",
    "        \"id\":\"mradermacher/patricide-12B-Unslop-Mell-GGUF\",\n",
    "        \"file\": \"https://huggingface.co/mradermacher/patricide-12B-Unslop-Mell-GGUF/blob/main/patricide-12B-Unslop-Mell.Q6_K.gguf\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PRESET = \"5\" #@param [1,2,3,4,5]\n",
    "\n",
    "MODEL = OPTIONS[int(PRESET)][\"file\"].replace(\"/blob/\", \"/resolve/\")\n",
    "\n",
    "!aria2c -x 10 -o models/model.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@LAUNCH!\n",
    "\n",
    "MODEL_PATH = r\"models/model.gguf\"\n",
    "\n",
    "GPU_LAYERS = 80 #@param\n",
    "CONTEXT_SIZE = 8192 #@param\n",
    "\n",
    "!./server -m $MODEL_PATH --host 0.0.0.0 --port 8000 --threads 4 --ctx-size $CONTEXT_SIZE --gpu-layers $GPU_LAYERS --mlock --log-disable & pylt port 8000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
