{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUaatkGgzt2"
      },
      "outputs": [],
      "source": [
        "#@title Prevent disconnections\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Load utils\n",
        "\n",
        "globals()[\"SOURCE\"] = \"oobabooga\"\n",
        "globals()[\"NAME\"] = \"text-generation-webui\"\n",
        "\n",
        "BASE = f\"https://github.com/{globals()['SOURCE']}/{globals()['NAME']}\"\n",
        "BASE_PATH = f\"/content/{globals()['NAME']}\"\n",
        "# NGROK = f\"/content/{globals()['NAME']}/extensions/ngrok/requirements.txt\"\n",
        "\n",
        "\n",
        "def get_renamed_file(MODEL_TYPE, MODEL_FILES, links):\n",
        "    if MODEL_TYPE == \"gguf\":\n",
        "        truth_links = MODEL_FILES if len(MODEL_FILES) > 0 else links\n",
        "        file = list(filter(lambda filename: filename.endswith(\".gguf\"), truth_links))[0]\n",
        "        return file.split(file[file.rfind(\"/\")])[-1]\n",
        "\n",
        "    if MODEL_TYPE == \"gptq\":\n",
        "        return \"gptq_model-4bit-128g.safetensors\"\n",
        "\n",
        "    if MODEL_TYPE == \"exl2\":\n",
        "        return \"output.safetensors\"\n",
        "    \n",
        "\n",
        "def get_target_repo(MODEL_ID):\n",
        "    return f\"https://huggingface.co/{MODEL_ID}/tree/main?not-for-all-audiences=true\"\n",
        "\n",
        "\n",
        "def get_target_links(MODEL_TYPE, MODEL_FILES, links):\n",
        "\n",
        "    LINKS = []\n",
        "\n",
        "    if MODEL_TYPE == \"gguf\":\n",
        "        truth_links = MODEL_FILES if len(MODEL_FILES) > 0 else links\n",
        "        LINKS = [link.replace(\"/blob/\", \"/resolve/\") for link in truth_links if link.endswith(\".gguf\") and \"/blob/\" in link]\n",
        "        return LINKS\n",
        "\n",
        "    FILE_TARGETS = (\".json\", \".model\", \".safetensors\", \".yml\", \".py\")\n",
        "\n",
        "    if len(MODEL_FILES) > 0:\n",
        "        LINKS = MODEL_FILES\n",
        "        for item in LINKS:\n",
        "            item = item.replace(\"/blob/\", \"/resolve/\")\n",
        "    else:\n",
        "        for link in links:\n",
        "            if (\n",
        "                any(link.endswith(target) for target in FILE_TARGETS)\n",
        "                and \"/resolve/\" in link\n",
        "            ):\n",
        "                LINKS.append(link)\n",
        "\n",
        "    return LINKS\n",
        "\n",
        "\n",
        "def create_download_file(MODEL_TYPE, LINKS, shopping_list, renamed_file):\n",
        "    with open(shopping_list, \"a\") as file:\n",
        "        if MODEL_TYPE == \"gguf\":\n",
        "            for url in LINKS:\n",
        "                file.write(f\"{url}\\n out={renamed_file}\\n\")\n",
        "        else:\n",
        "            for url in LINKS:\n",
        "                if url.endswith(\".safetensors\"):\n",
        "                    file.write(f\"{url}\\n out={renamed_file}\\n\")\n",
        "                elif url.endswith(\".model\"):\n",
        "                    file.write(f\"{url}\\n out=tokenizer.model\\n\")\n",
        "                else:\n",
        "                    file.write(f\"{url}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "#@title Download & install backend\n",
        "!apt-get -y install -qq aria2\n",
        "#!pip install flask-cloudflared\n",
        "!pip install requests-html\n",
        "\n",
        "!git clone $BASE\n",
        "%cd $BASE_PATH\n",
        "!pip install -r requirements.txt\n",
        "#!pip install -r $NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p7V3zf_gzt3"
      },
      "outputs": [],
      "source": [
        "#@title Add model\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "from requests_html import HTMLSession\n",
        "\n",
        "OPTIONS = {\n",
        "    1: {\n",
        "        \"id\": \"TheBloke/Echidna-13B-v0.3-GPTQ\", \n",
        "        \"file_list\": []\n",
        "    },\n",
        "    2: {\n",
        "        \"id\": \"mradermacher/Fimbulvetr-11B-v2-GGUF\",\n",
        "        \"file_list\": [\n",
        "            \"https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-GGUF/blob/main/Fimbulvetr-11B-v2.Q5_K_S.gguf\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "#@markdown ### Model presets\n",
        "Preset = \"2\" #@param [1,2]\n",
        "\n",
        "OPTION = OPTIONS[int(Preset)]\n",
        "MODEL_ID = OPTION[\"id\"]\n",
        "MODEL_FILES = OPTION[\"file_list\"]\n",
        "MODEL_NAME = MODEL_ID.split(MODEL_ID[MODEL_ID.rfind(\"/\")])[-1]\n",
        "\n",
        "#@markdown ### Select model type\n",
        "MODEL_TYPE = \"gguf\" #@param [\"gguf\", \"gtpq\", \"exl2\"]\n",
        "\n",
        " #Target repo\n",
        "REPO = get_target_repo(MODEL_ID)\n",
        "\n",
        "#Get file urls\n",
        "session = HTMLSession()\n",
        "result = session.get(REPO)\n",
        "links = result.html.absolute_links\n",
        "\n",
        "#@markdown ### model file name based on backend and model type (empty by default)\n",
        "renamed_file = \"\" #@param {type:\"string\"}\n",
        "renamed_file = renamed_file or get_renamed_file(MODEL_TYPE, MODEL_FILES, links)\n",
        "\n",
        "#Set formatted LINKS list\n",
        "LINKS = get_target_links(MODEL_TYPE, MODEL_FILES, links)\n",
        "\n",
        "#Input file for aria2\n",
        "shopping_list = \"down_list.txt\"\n",
        "\n",
        "#Create file for aria2 using LINKS list\n",
        "create_download_file(MODEL_TYPE, LINKS, shopping_list, renamed_file)\n",
        "\n",
        "#Set models destination for backend\n",
        "models_folder = f\"{BASE_PATH}/models\"\n",
        "\n",
        "#Download all model config files\n",
        "print(f\"Downloading {MODEL_NAME}...\\n\")\n",
        "\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    !cd $models_folder\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --summary-interval=5 --input-file=$shopping_list -d $models_folder\n",
        "    !rm $shopping_list\n",
        "\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    !cd $models_folder && mkdir $MODEL_NAME\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --summary-interval=5 --input-file=$shopping_list -d $models_folder/$MODEL_NAME\n",
        "    !rm $shopping_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6BfvYTgzt3"
      },
      "outputs": [],
      "source": [
        "#@title LAUNCH! (Run again in case of cloudflare error)\n",
        "\n",
        "%cd $BASE_PATH\n",
        "\n",
        "provider = \"cloudflare\" #@param [\"ngrok\", \"cloudflare\"]\n",
        "context = \"4096\" #@param {type:\"string\"}\n",
        "gpu_layers = \"60\" #@param {type:\"string\"}\n",
        "\n",
        "CONTEXT = int(context)\n",
        "GPU_LAYERS = int(gpu_layers)\n",
        "\n",
        "if provider == \"ngrok\":\n",
        "    PROVIDER = \"--extension ngrok\"\n",
        "else:\n",
        "    PROVIDER = \"\"\n",
        "\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    LOADER = \"LLAMACPP\"\n",
        "    MODEL_NAME = renamed_file\n",
        "    !python server.py --share --nowebui --api --public-api $PROVIDER --model $MODEL_NAME --loader $LOADER --max_seq_len $CONTEXT --n_ctx $CONTEXT --n-gpu-layers $GPU_LAYERS\n",
        "\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    LOADER = \"EXLLAMAV2_HF\"\n",
        "    !python server.py --share --nowebui --api --public-api $PROVIDER --model $MODEL_NAME --loader $LOADER --max_seq_len $CONTEXT --n_ctx $CONTEXT"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
